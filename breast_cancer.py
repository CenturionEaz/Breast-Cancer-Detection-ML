# -*- coding: utf-8 -*-
"""Breast_Cancer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fKJdXd-dxWq_Xumh2XnlepLSO2c5_2an

# **Exploring The Dataset**
"""

# Import necessary libraries
import pandas as pd

# Load the dataset
file_path = "/content/data.csv"
df = pd.read_csv(file_path)

# 1. Display the first few rows of the dataset
first_few_rows = df.head()

# 2. Get basic statistical summary of the dataset
statistical_summary = df.describe()

# 3. Check for missing values in the dataset
missing_values = df.isnull().sum()

# 4. Drop the 'ID' column and the last column (Unnamed: 32)
df_cleaned = df.drop(columns=['id', 'Unnamed: 32'])

# Output the results
first_few_rows, statistical_summary, missing_values

df_cleaned['diagnosis'] = df_cleaned['diagnosis'].map({'M': 1, 'B': 0})

import matplotlib.pyplot as plt
import seaborn as sns

# 1. Plot the distribution of the target variable (diagnosis)
plt.figure(figsize=(6, 4))
sns.countplot(data=df_cleaned, x='diagnosis', palette='Set2')
plt.title("Distribution of Diagnosis (Benign vs Malignant)")
plt.xlabel("Diagnosis")
plt.ylabel("Count")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# List of features to plot
features_to_plot = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean']

# Set up the plotting area
plt.figure(figsize=(12, 8))
for i, feature in enumerate(features_to_plot, 1):
    plt.subplot(2, 2, i)
    sns.histplot(df_cleaned[feature], kde=True, color='skyblue')
    plt.title(f'Distribution of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# 3. Correlation Heatmap
plt.figure(figsize=(16, 12))

# Compute the correlation matrix
correlation_matrix = df_cleaned.corr()

# Generate the heatmap using Seaborn
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', linewidths=0.5)
plt.title("Correlation Heatmap of Features")
plt.show()

from statsmodels.stats.outliers_influence import variance_inflation_factor
import pandas as pd

# Prepare the features (excluding 'diagnosis' column)
X = df_cleaned.drop(columns=['diagnosis'])

# Initialize a DataFrame to store VIF scores
vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# Display VIF scores
print(vif_data.sort_values(by="VIF", ascending=False))

# Dropping features with very high VIF scores due to multicollinearity
features_to_drop = ['perimeter_mean', 'area_mean', 'perimeter_worst', 'area_worst']
df_reduced = df_cleaned.drop(columns=features_to_drop)

# Verify the changes by recalculating VIF
X_reduced = df_reduced.drop(columns=['diagnosis'])
vif_data_reduced = pd.DataFrame()
vif_data_reduced["Feature"] = X_reduced.columns
vif_data_reduced["VIF"] = [variance_inflation_factor(X_reduced.values, i) for i in range(X_reduced.shape[1])]

# Display reduced VIF scores
print(vif_data_reduced.sort_values(by="VIF", ascending=False))

from sklearn.preprocessing import StandardScaler

# Separate features and target
X = df_reduced.drop(columns=['diagnosis'])
y = df_reduced['diagnosis']

# Apply StandardScaler to normalize feature values
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Convert back to DataFrame for readability
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

# Display first few rows of the scaled data
print(X_scaled_df.head())

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Apply PCA
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Visualize explained variance
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),
         pca.explained_variance_ratio_.cumsum(), marker='o', linestyle='--')
plt.title('Cumulative Explained Variance by PCA Components')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.grid()
plt.show()

# Select the number of components to retain ~95% variance
pca = PCA(n_components=10)
X_pca_reduced = pca.fit_transform(X_scaled)

# Convert the PCA-transformed data back to a DataFrame for analysis
X_pca_df = pd.DataFrame(X_pca_reduced, columns=[f'PC{i+1}' for i in range(10)])

# Display the first few rows of the PCA-reduced dataset
print(X_pca_df.head())

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_pca_reduced, y, test_size=0.2, random_state=42)

# Initialize and train a Logistic Regression model
model = LogisticRegression(random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Ensure proper train-test split for the PCA-transformed data
X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca_reduced, y, test_size=0.2, random_state=42)

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Get the predicted probabilities for the positive class
y_proba = model.predict_proba(X_test_pca)[:, 1]

# Compute the ROC curve and AUC score
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
auc_score = roc_auc_score(y_test, y_proba)

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"Logistic Regression (AUC = {auc_score:.2f})", linewidth=2)
plt.plot([0, 1], [0, 1], linestyle="--", color="gray", label="Random Guess")
plt.title("Receiver Operating Characteristic (ROC) Curve", fontsize=14)
plt.xlabel("False Positive Rate", fontsize=12)
plt.ylabel("True Positive Rate", fontsize=12)
plt.legend(loc="lower right")
plt.grid(alpha=0.4)
plt.show()

from sklearn.metrics import precision_recall_curve

# Get predicted probabilities for the positive class
y_proba = model.predict_proba(X_test_pca)[:, 1]

# Calculate precision, recall, and thresholds
precision, recall, thresholds = precision_recall_curve(y_test, y_proba)

# Calculate F1-score for each threshold
f1_scores = 2 * (precision * recall) / (precision + recall)

# Find the optimal threshold (maximize F1-score)
optimal_idx = f1_scores.argmax()
optimal_threshold = thresholds[optimal_idx]

print(f"Optimal Threshold: {optimal_threshold:.2f}")
print(f"Precision: {precision[optimal_idx]:.2f}, Recall: {recall[optimal_idx]:.2f}")

# Predict using the new threshold
y_pred_tuned = (y_proba >= optimal_threshold).astype(int)

# Evaluate with the tuned threshold
print("\nClassification Report with Tuned Threshold:")
print(classification_report(y_test, y_pred_tuned))

import joblib

# Save the PCA transformer
pca_file = "pca_pipeline.pkl"
joblib.dump(pca, pca_file)

# Save the trained classifier
model_file = "logistic_regression_model.pkl"
joblib.dump(model, model_file)

print(f"Model saved as {model_file}")
print(f"PCA pipeline saved as {pca_file}")

# Load the saved PCA and model
loaded_pca = joblib.load("/content/pca_pipeline.pkl")
loaded_model = joblib.load("/content/logistic_regression_model.pkl")

# Transform new data using PCA
X_new_pca = loaded_pca.transform(X_new)

# Predict using the loaded model
predictions = loaded_model.predict(X_new_pca)
print("Predictions:", predictions)